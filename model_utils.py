import re
from typing import List, Union, Dict, Optional

import numpy as np
import torch.nn as nn
from torch.nn.modules.conv import _ConvNd

from common_utils import to


__all__ = [
    'LINEAR_LAYERS',
    "ForwardInterrupt", 
    "InputCollector"
    "select_layers",
    "get_number_of_rows_and_cols"
]


LINEAR_LAYERS = (nn.Linear, _ConvNd)


class ForwardInterrupt(Exception):
    pass


class InputCollector(nn.Module):
    
    def __init__(self, module: nn.Module, cpu_offload: bool = False):
        super().__init__()
        self.module = module
        self.cpu_offload = cpu_offload
        self.input_args = []
        self.input_kwargs = []

    def forward(self, *input_args, **input_kwargs):
        """
        Assumes that the wrapped module has a single 
        input that can reside in inputs or input_kwargs.
        """
        if self.cpu_offload:
            input_args = to(input_args, device='cpu')
            input_kwargs = to(input_kwargs, device='cpu')
        self.input_args.append(input_args)
        self.input_kwargs.append(input_kwargs)
        raise ForwardInterrupt


def select_layers(
    model: nn.Module, 
    layer_prefix: Optional[str] = '',
    layer_regex: str = '.*', 
    layer_classes: Union[nn.Module, List[nn.Module]] = nn.Module
) -> Dict[str, nn.Module]:
    layers = {}
    for layer_name, layer in model.named_modules():
        if isinstance(layer, layer_classes) and re.search(layer_regex, layer_name) and layer_name.startswith(layer_prefix):
            layers[layer_name] = layer
    return layers


def get_number_of_rows_and_cols(layer):
    return layer.weight.shape[0], np.prod(layer.weight.shape[1:])
